{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxicity identier\n",
    "\n",
    "I want to create a model that will access the level of toxicity, and use this score as metric (it can be considered as part of [J metric](https://aclanthology.org/2022.acl-long.469.pdf))  or add it to the loss. I will train the simple model with pretrained tokenizer from the model for final solution, embedding layer, several linear layers and output with sigmoid function. \n",
    "\n",
    "Model should have fast inference time, not consume a lot of resources and have normal performance(this is not a critical parameter because I prioritize speed), but should still produce roughly similar levels of toxicity.\n",
    "\n",
    "Also, I will convert tox_level to integer values and train the model on a classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T18:39:35.850775600Z",
     "start_time": "2023-11-03T18:39:32.044501200Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.utils.data as data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T18:39:35.873367900Z",
     "start_time": "2023-11-03T18:39:35.849311400Z"
    }
   },
   "outputs": [],
   "source": [
    "!unzip ../data/interim/toxicity_levels.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T18:39:37.106242400Z",
     "start_time": "2023-11-03T18:39:35.873367900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tox_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>if Alkar floods her with her mental waste, it ...</td>\n",
       "      <td>0.981983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>If Alkar is flooding her with psychic waste, t...</td>\n",
       "      <td>0.014195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you're becoming disgusting.</td>\n",
       "      <td>0.999039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Now you're getting nasty.</td>\n",
       "      <td>0.065473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>well, we can spare your life.</td>\n",
       "      <td>0.985068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  tox_level\n",
       "0  if Alkar floods her with her mental waste, it ...   0.981983\n",
       "1  If Alkar is flooding her with psychic waste, t...   0.014195\n",
       "2                        you're becoming disgusting.   0.999039\n",
       "3                          Now you're getting nasty.   0.065473\n",
       "4                      well, we can spare your life.   0.985068"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/interim/toxicity_levels.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T18:39:37.293863700Z",
     "start_time": "2023-11-03T18:39:37.102718900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tox_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>if Alkar floods her with her mental waste, it ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>If Alkar is flooding her with psychic waste, t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you're becoming disgusting.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Now you're getting nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>well, we can spare your life.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  tox_level\n",
       "0  if Alkar floods her with her mental waste, it ...          1\n",
       "1  If Alkar is flooding her with psychic waste, t...          0\n",
       "2                        you're becoming disgusting.          1\n",
       "3                          Now you're getting nasty.          0\n",
       "4                      well, we can spare your life.          1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "\n",
    "df['tox_level'] = df['tox_level'].apply(lambda x: 1 if x > threshold else 0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T18:39:44.057358800Z",
     "start_time": "2023-11-03T18:39:37.295870100Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_checkpoint = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T18:39:44.072127500Z",
     "start_time": "2023-11-03T18:39:44.059370100Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessing_stage(sample):\n",
    "    # in the preprocessing phase, I convert the input text to the list of tokens\n",
    "    model_inputs = tokenizer(sample['text'], padding='max_length', max_length=256, truncation=True)\n",
    "    return model_inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T18:41:21.047697800Z",
     "start_time": "2023-11-03T18:39:44.063386Z"
    }
   },
   "outputs": [],
   "source": [
    "df['input_ids'] = df.apply(lambda x: preprocessing_stage(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T18:41:21.094867400Z",
     "start_time": "2023-11-03T18:41:21.051712300Z"
    }
   },
   "outputs": [],
   "source": [
    "df.drop(columns=['text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T18:41:23.596186400Z",
     "start_time": "2023-11-03T18:41:21.075579700Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ratio = 0.2\n",
    "train, val = train_test_split(\n",
    "    df, stratify=df['tox_level'], test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T18:41:23.607651900Z",
     "start_time": "2023-11-03T18:41:23.596186400Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    text_list, toxicity_list = [], []\n",
    "    for _toxicity, _text in batch:\n",
    "        text_list.append(_text)\n",
    "        toxicity_list.append(_toxicity)\n",
    "    return torch.LongTensor(text_list).to(device), torch.FloatTensor(toxicity_list).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T18:41:23.653779300Z",
     "start_time": "2023-11-03T18:41:23.604138Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "train_dataloader = data_utils.DataLoader(\n",
    "    train.to_numpy(), batch_size=batch_size, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "val_dataloader = data_utils.DataLoader(\n",
    "    val.to_numpy(), batch_size=batch_size, shuffle=False, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T10:06:48.319524Z",
     "start_time": "2023-11-04T10:06:27.358925900Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from src.utils.trainer import Trainer\n",
    "from src.models.architectures.toxicity_classification_model import ToxicityClassificationModel\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T10:06:55.749094500Z",
     "start_time": "2023-11-04T10:06:55.440782700Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_size = 32128\n",
    "\n",
    "model = ToxicityClassificationModel(vocab_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()\n",
    "trainer = Trainer(ToxicityClassificationModel(vocab_size), 'pytorch', device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T18:44:08.646021700Z",
     "start_time": "2023-11-03T18:44:02.579780600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: train:   0%|          | 0/1145 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: train: 100%|██████████| 1145/1145 [00:29<00:00, 39.02it/s, loss=0.0068] \n",
      "Epoch 1: val: 100%|██████████| 287/287 [00:04<00:00, 61.96it/s, loss=0.00136]\n",
      "Epoch 2: train: 100%|██████████| 1145/1145 [00:22<00:00, 50.95it/s, loss=0.0068] \n",
      "Epoch 2: val: 100%|██████████| 287/287 [00:04<00:00, 71.46it/s, loss=0.00136]\n",
      "Epoch 3: train: 100%|██████████| 1145/1145 [00:22<00:00, 50.18it/s, loss=0.0068] \n",
      "Epoch 3: val: 100%|██████████| 287/287 [00:04<00:00, 65.30it/s, loss=0.00136]\n",
      "Epoch 4: train: 100%|██████████| 1145/1145 [00:22<00:00, 50.37it/s, loss=0.0068] \n",
      "Epoch 4: val: 100%|██████████| 287/287 [00:04<00:00, 65.94it/s, loss=0.00136]\n",
      "Epoch 5: train: 100%|██████████| 1145/1145 [00:22<00:00, 51.02it/s, loss=0.0068] \n",
      "Epoch 5: val: 100%|██████████| 287/287 [00:04<00:00, 61.58it/s, loss=0.00136]\n",
      "Epoch 6: train: 100%|██████████| 1145/1145 [00:22<00:00, 51.92it/s, loss=0.0068] \n",
      "Epoch 6: val: 100%|██████████| 287/287 [00:04<00:00, 66.68it/s, loss=0.00136]\n",
      "Epoch 7: train: 100%|██████████| 1145/1145 [00:22<00:00, 50.60it/s, loss=0.0068] \n",
      "Epoch 7: val: 100%|██████████| 287/287 [00:04<00:00, 66.51it/s, loss=0.00136]\n",
      "Epoch 8: train: 100%|██████████| 1145/1145 [00:22<00:00, 51.47it/s, loss=0.0068] \n",
      "Epoch 8: val: 100%|██████████| 287/287 [00:04<00:00, 66.83it/s, loss=0.00136]\n",
      "Epoch 9: train: 100%|██████████| 1145/1145 [00:22<00:00, 52.04it/s, loss=0.0068] \n",
      "Epoch 9: val: 100%|██████████| 287/287 [00:04<00:00, 69.20it/s, loss=0.00136]\n",
      "Epoch 10: train: 100%|██████████| 1145/1145 [00:22<00:00, 51.16it/s, loss=0.0068] \n",
      "Epoch 10: val: 100%|██████████| 287/287 [00:04<00:00, 64.58it/s, loss=0.00136]\n",
      "Epoch 11: train: 100%|██████████| 1145/1145 [00:22<00:00, 50.21it/s, loss=0.0068] \n",
      "Epoch 11: val: 100%|██████████| 287/287 [00:04<00:00, 68.20it/s, loss=0.00136]\n",
      "Epoch 12: train: 100%|██████████| 1145/1145 [00:22<00:00, 51.64it/s, loss=0.0068] \n",
      "Epoch 12: val: 100%|██████████| 287/287 [00:04<00:00, 68.23it/s, loss=0.00136]\n",
      "Epoch 13: train: 100%|██████████| 1145/1145 [00:22<00:00, 51.90it/s, loss=0.0068] \n",
      "Epoch 13: val: 100%|██████████| 287/287 [00:04<00:00, 63.28it/s, loss=0.00136]\n",
      "Epoch 14: train: 100%|██████████| 1145/1145 [00:22<00:00, 51.51it/s, loss=0.0068] \n",
      "Epoch 14: val: 100%|██████████| 287/287 [00:04<00:00, 69.11it/s, loss=0.00136]\n",
      "Epoch 15: train: 100%|██████████| 1145/1145 [00:22<00:00, 51.65it/s, loss=0.0068] \n",
      "Epoch 15: val: 100%|██████████| 287/287 [00:04<00:00, 67.78it/s, loss=0.00136]\n",
      "Epoch 16: train: 100%|██████████| 1145/1145 [00:23<00:00, 49.73it/s, loss=0.0068] \n",
      "Epoch 16: val: 100%|██████████| 287/287 [00:04<00:00, 66.64it/s, loss=0.00136]\n",
      "Epoch 17: train: 100%|██████████| 1145/1145 [00:21<00:00, 52.89it/s, loss=0.0068] \n",
      "Epoch 17: val: 100%|██████████| 287/287 [00:04<00:00, 68.40it/s, loss=0.00136]\n",
      "Epoch 18: train: 100%|██████████| 1145/1145 [00:23<00:00, 49.76it/s, loss=0.0068] \n",
      "Epoch 18: val: 100%|██████████| 287/287 [00:03<00:00, 72.50it/s, loss=0.00136]\n",
      "Epoch 19: train: 100%|██████████| 1145/1145 [00:22<00:00, 50.88it/s, loss=0.0068] \n",
      "Epoch 19: val: 100%|██████████| 287/287 [00:04<00:00, 70.21it/s, loss=0.00136]\n"
     ]
    }
   ],
   "source": [
    "trainer.train(20, optimizer=optimizer, loss_fn=criterion, train_dataloader=train_dataloader, use_validation=True, val_dataloader=val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scripted = torch.jit.script(model)\n",
    "model_scripted.save('../models/toxicity_identifier.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_toxicity(model, inference_result, tokenizer=tokenizer):\n",
    "    input_ids = tokenizer(inference_result, return_tensors=\"pt\").input_ids\n",
    "    print(model(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4912]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.4908]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.4896]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.4965]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "check_toxicity(model, \"I love you so much\")\n",
    "check_toxicity(model, \"I'm famous, and you're dead\")\n",
    "check_toxicity(model, \"And it just helped that you have no morals or integrity\")\n",
    "check_toxicity(model, \"Nolan will destroy it\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running some manual tests, I can conclude that this model and its minor modifications do not work as I expected(can't differentiate the toxic and nontoxic phrases, and often produce values around 0.5)."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exploring internet\n",
    "\n",
    "After investigating [1](https://arxiv.org/pdf/2109.08914.pdf) and [2](https://aclanthology.org/2022.acl-long.469.pdf), I found that authors used [this model](https://huggingface.co/s-nlp/roberta_toxicity_classifier) and tried it in my case."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from src.utils.predictor import Predictor"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T16:56:19.203472600Z",
     "start_time": "2023-11-04T16:56:15.259689100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at s-nlp/roberta_toxicity_classifier_v1 were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "predictor = Predictor('s-nlp/roberta_toxicity_classifier_v1', 'transformers', 'classification')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T16:56:22.106483600Z",
     "start_time": "2023-11-04T16:56:19.205664300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.859886005055159e-05\n",
      "0.9121143221855164\n",
      "0.6886491179466248\n",
      "0.7692179083824158\n"
     ]
    }
   ],
   "source": [
    "print(predictor.predict('I love you so much'))\n",
    "print(predictor.predict(\"I'm famous, and you're dead\"))\n",
    "print(predictor.predict('And it just helped that you have no morals or integrity'))\n",
    "print(predictor.predict('Nolan will destroy it'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-04T16:56:22.543480200Z",
     "start_time": "2023-11-04T16:56:22.107480800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "These results suit me and I will use this model in J metric"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
