{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity embeddings\n",
    "\n",
    "In this notebook, I will compare different embedding algorithms to define which is better for calculating content preservation. It is needed to preserve the meaning of the replaced word.\n",
    "For determing the best one, I will divide dataset in train and validation parts, train the models(except transformers) and calculate the MSE for cosine similarity for the toxic and non-toxic sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "random_state = 0\n",
    "\n",
    "random.seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "torch.manual_seed(random_state)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(random_state)\n",
    "transformers.set_seed(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data/interim/toxicity_levels.csv')\n",
    "df_val = pd.read_csv('../data/raw/filtered.tsv', sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df_val(df):\n",
    "    df['toxic'] = df.apply(lambda x: x['reference'] if x['ref_tox'] > x['trn_tox'] else x['translation'], axis=1)\n",
    "    df['detoxified'] = df.apply(lambda x: x[\"translation\"] if x['ref_tox'] > x['trn_tox'] else x['reference'], axis=1)\n",
    "\n",
    "    return df[['toxic', 'detoxified', 'similarity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = preprocess_df_val(df_val.sample(20000, random_state=random_state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "\n",
    "\n",
    "class MyCorpus:\n",
    "    def __iter__(self):\n",
    "        for line in df_train.iterrows():\n",
    "            yield utils.simple_preprocess(line[1].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models\n",
    "\n",
    "sentences = MyCorpus()\n",
    "model = gensim.models.Word2Vec(sentences=sentences, vector_size=200, workers=8, seed=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"../models/word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text'].to_csv('data.txt', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 8M words\n",
      "Number of words:  55928\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  100734 lr:  0.000000 avg.loss:  2.040466 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "\n",
    "model = fasttext.train_unsupervised('data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"../models/fasttext.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_word2vec = gensim.models.Word2Vec.load('../models/word2vec.model')\n",
    "se_score_word2vec = []\n",
    "\n",
    "for row in df_val.iterrows():\n",
    "    embedding1 = np.zeros(200)\n",
    "    for word in row[1].toxic.split():\n",
    "        embedding1 += model_word2vec.wv[word] if word in model_word2vec.wv else np.zeros(200)\n",
    "    embedding2 = np.zeros(200)\n",
    "    for word in row[1].detoxified.split():\n",
    "        embedding2 += model_word2vec.wv[word] if word in model_word2vec.wv else np.zeros(200)\n",
    "    score = cosine_similarity(embedding1.reshape(1, -1), embedding2.reshape(1, -1))\n",
    "    se_score_word2vec.append(norm(score - row[1].similarity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "model_fasttext = fasttext.load_model('../models/fasttext.bin')\n",
    "se_score_fasttext = []\n",
    "\n",
    "for row in df_val.iterrows():\n",
    "    embedding1 = np.zeros(100)\n",
    "    for word in row[1].toxic.split():\n",
    "        embedding1 += model_fasttext[word]\n",
    "    embedding2 = np.zeros(100)\n",
    "    for word in row[1].detoxified.split():\n",
    "        embedding2 += model_fasttext[word]\n",
    "    score = cosine_similarity(embedding1.reshape(1, -1), embedding2.reshape(1, -1))\n",
    "    se_score_fasttext.append(norm(score - row[1].similarity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(sentences):\n",
    "    inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state\n",
    "    return embeddings[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20000it [17:21, 19.21it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "se_score_bert = []\n",
    "\n",
    "for row in tqdm(df_val.iterrows()):\n",
    "    embedding1, embedding2 = get_embeddings([row[1].toxic, row[1].detoxified])\n",
    "    score = cosine_similarity(embedding1.reshape(1, -1), embedding2.reshape(1, -1))\n",
    "    se_score_bert.append(norm(score - row[1].similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec MSE =  0.21221232225860703\n",
      "Fasttext MSE =  0.13669340326418633\n",
      "Bert MSE =  0.18090047\n"
     ]
    }
   ],
   "source": [
    "print('Word2Vec MSE = ', np.array(se_score_word2vec).mean())  # 9.5s\n",
    "print('Fasttext MSE = ', np.array(se_score_fasttext).mean())  # 12.9s\n",
    "print('Bert MSE = ', np.array(se_score_bert).mean())  # 17m 21.1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results, I can conclude that Fasstext has the best performance in term of MSE, but it little bit slower than Word2Vec and uses a lot of disk space. Bert shows very slow performance in compare with Word2Vec and Fasttext.\n",
    "\n",
    "For cosine similarity I will use fasttext embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
